{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wU3HmTEDCt19"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TnG-kwDDNLf"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgQRKiPNDWgy"
      },
      "outputs": [],
      "source": [
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalization\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaeJid48DZOV"
      },
      "outputs": [],
      "source": [
        "dataset = ImageFolder('Spec',transform=data_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMuoLtmYDzqB"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "# Split the dataset into train and test subsets\n",
        "train_set, test_set = random_split(dataset, [train_size, test_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JW3K4pGOD0bU"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mXc3-RsD2Hn"
      },
      "outputs": [],
      "source": [
        "classes = dataset.classes\n",
        "num_emotions = len(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y9vJ4HYHDUb",
        "outputId": "4646c225-2a0e-4ad8-a1c8-c2d9278fdd35"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OQhTYVuD3il",
        "outputId": "78ec9adc-1589-4fe6-88af-ceb069a217ea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = models.resnet50(pretrained=True)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkQuokENEg0-"
      },
      "outputs": [],
      "source": [
        "num_features = model.fc.in_features     #extract fc layers features\n",
        "model.fc = nn.Linear(num_features, num_emotions)\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1RQPf4kEh1Z",
        "outputId": "2a30fd48-fce5-44fd-f462-b8ed35b7d21a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "conv1.weight\n",
            "bn1.weight\n",
            "bn1.bias\n",
            "layer1.0.conv1.weight\n",
            "layer1.0.bn1.weight\n",
            "layer1.0.bn1.bias\n",
            "layer1.0.conv2.weight\n",
            "layer1.0.bn2.weight\n",
            "layer1.0.bn2.bias\n",
            "layer1.0.conv3.weight\n",
            "layer1.0.bn3.weight\n",
            "layer1.0.bn3.bias\n",
            "layer1.0.downsample.0.weight\n",
            "layer1.0.downsample.1.weight\n",
            "layer1.0.downsample.1.bias\n",
            "layer1.1.conv1.weight\n",
            "layer1.1.bn1.weight\n",
            "layer1.1.bn1.bias\n",
            "layer1.1.conv2.weight\n",
            "layer1.1.bn2.weight\n",
            "layer1.1.bn2.bias\n",
            "layer1.1.conv3.weight\n",
            "layer1.1.bn3.weight\n",
            "layer1.1.bn3.bias\n",
            "layer1.2.conv1.weight\n",
            "layer1.2.bn1.weight\n",
            "layer1.2.bn1.bias\n",
            "layer1.2.conv2.weight\n",
            "layer1.2.bn2.weight\n",
            "layer1.2.bn2.bias\n",
            "layer1.2.conv3.weight\n",
            "layer1.2.bn3.weight\n",
            "layer1.2.bn3.bias\n",
            "layer2.0.conv1.weight\n",
            "layer2.0.bn1.weight\n",
            "layer2.0.bn1.bias\n",
            "layer2.0.conv2.weight\n",
            "layer2.0.bn2.weight\n",
            "layer2.0.bn2.bias\n",
            "layer2.0.conv3.weight\n",
            "layer2.0.bn3.weight\n",
            "layer2.0.bn3.bias\n",
            "layer2.0.downsample.0.weight\n",
            "layer2.0.downsample.1.weight\n",
            "layer2.0.downsample.1.bias\n",
            "layer2.1.conv1.weight\n",
            "layer2.1.bn1.weight\n",
            "layer2.1.bn1.bias\n",
            "layer2.1.conv2.weight\n",
            "layer2.1.bn2.weight\n",
            "layer2.1.bn2.bias\n",
            "layer2.1.conv3.weight\n",
            "layer2.1.bn3.weight\n",
            "layer2.1.bn3.bias\n",
            "layer2.2.conv1.weight\n",
            "layer2.2.bn1.weight\n",
            "layer2.2.bn1.bias\n",
            "layer2.2.conv2.weight\n",
            "layer2.2.bn2.weight\n",
            "layer2.2.bn2.bias\n",
            "layer2.2.conv3.weight\n",
            "layer2.2.bn3.weight\n",
            "layer2.2.bn3.bias\n",
            "layer2.3.conv1.weight\n",
            "layer2.3.bn1.weight\n",
            "layer2.3.bn1.bias\n",
            "layer2.3.conv2.weight\n",
            "layer2.3.bn2.weight\n",
            "layer2.3.bn2.bias\n",
            "layer2.3.conv3.weight\n",
            "layer2.3.bn3.weight\n",
            "layer2.3.bn3.bias\n",
            "layer3.0.conv1.weight\n",
            "layer3.0.bn1.weight\n",
            "layer3.0.bn1.bias\n",
            "layer3.0.conv2.weight\n",
            "layer3.0.bn2.weight\n",
            "layer3.0.bn2.bias\n",
            "layer3.0.conv3.weight\n",
            "layer3.0.bn3.weight\n",
            "layer3.0.bn3.bias\n",
            "layer3.0.downsample.0.weight\n",
            "layer3.0.downsample.1.weight\n",
            "layer3.0.downsample.1.bias\n",
            "layer3.1.conv1.weight\n",
            "layer3.1.bn1.weight\n",
            "layer3.1.bn1.bias\n",
            "layer3.1.conv2.weight\n",
            "layer3.1.bn2.weight\n",
            "layer3.1.bn2.bias\n",
            "layer3.1.conv3.weight\n",
            "layer3.1.bn3.weight\n",
            "layer3.1.bn3.bias\n",
            "layer3.2.conv1.weight\n",
            "layer3.2.bn1.weight\n",
            "layer3.2.bn1.bias\n",
            "layer3.2.conv2.weight\n",
            "layer3.2.bn2.weight\n",
            "layer3.2.bn2.bias\n",
            "layer3.2.conv3.weight\n",
            "layer3.2.bn3.weight\n",
            "layer3.2.bn3.bias\n",
            "layer3.3.conv1.weight\n",
            "layer3.3.bn1.weight\n",
            "layer3.3.bn1.bias\n",
            "layer3.3.conv2.weight\n",
            "layer3.3.bn2.weight\n",
            "layer3.3.bn2.bias\n",
            "layer3.3.conv3.weight\n",
            "layer3.3.bn3.weight\n",
            "layer3.3.bn3.bias\n",
            "layer3.4.conv1.weight\n",
            "layer3.4.bn1.weight\n",
            "layer3.4.bn1.bias\n",
            "layer3.4.conv2.weight\n",
            "layer3.4.bn2.weight\n",
            "layer3.4.bn2.bias\n",
            "layer3.4.conv3.weight\n",
            "layer3.4.bn3.weight\n",
            "layer3.4.bn3.bias\n",
            "layer3.5.conv1.weight\n",
            "layer3.5.bn1.weight\n",
            "layer3.5.bn1.bias\n",
            "layer3.5.conv2.weight\n",
            "layer3.5.bn2.weight\n",
            "layer3.5.bn2.bias\n",
            "layer3.5.conv3.weight\n",
            "layer3.5.bn3.weight\n",
            "layer3.5.bn3.bias\n",
            "layer4.0.conv1.weight\n",
            "layer4.0.bn1.weight\n",
            "layer4.0.bn1.bias\n",
            "layer4.0.conv2.weight\n",
            "layer4.0.bn2.weight\n",
            "layer4.0.bn2.bias\n",
            "layer4.0.conv3.weight\n",
            "layer4.0.bn3.weight\n",
            "layer4.0.bn3.bias\n",
            "layer4.0.downsample.0.weight\n",
            "layer4.0.downsample.1.weight\n",
            "layer4.0.downsample.1.bias\n",
            "layer4.1.conv1.weight\n",
            "layer4.1.bn1.weight\n",
            "layer4.1.bn1.bias\n",
            "layer4.1.conv2.weight\n",
            "layer4.1.bn2.weight\n",
            "layer4.1.bn2.bias\n",
            "layer4.1.conv3.weight\n",
            "layer4.1.bn3.weight\n",
            "layer4.1.bn3.bias\n",
            "layer4.2.conv1.weight\n",
            "layer4.2.bn1.weight\n",
            "layer4.2.bn1.bias\n",
            "layer4.2.conv2.weight\n",
            "layer4.2.bn2.weight\n",
            "layer4.2.bn2.bias\n",
            "layer4.2.conv3.weight\n",
            "layer4.2.bn3.weight\n",
            "layer4.2.bn3.bias\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad and 'fc' not in name:\n",
        "        print(name)\n",
        "        param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77rhgg5JEmii"
      },
      "outputs": [],
      "source": [
        "def plot_accuracy(train_accuracy,label=\"\"):\n",
        "    epochs = np.arange(1, len(train_accuracy) + 1)  # Create an array of epoch numbers\n",
        "\n",
        "    # Plotting the accuracy\n",
        "    plt.plot(epochs, train_accuracy, 'b', label=f'{label} Accuracy')\n",
        "    #plt.plot(epochs, val_accuracy, 'r', label='Validation Accuracy')\n",
        "    plt.title(f'{label} Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQX6NGVBEoQ4"
      },
      "outputs": [],
      "source": [
        "def train_model():\n",
        "  acc_valid = []\n",
        "  acc_train = []\n",
        "  num_epochs = 20   #(set no of epochs)\n",
        "  start_time = time.time() #(for showing time)\n",
        "  for epoch in range(num_epochs): #(loop for every epoch)\n",
        "      print(\"Epoch {} running\".format(epoch)) #(printing message)\n",
        "      \"\"\" Training Phase \"\"\"\n",
        "      model.train()    #(training model)\n",
        "      running_loss = 0.   #(set loss 0)\n",
        "      running_corrects = 0\n",
        "      # load a batch data of images\n",
        "      with tqdm(train_loader,unit=\"batch\") as tepoch:\n",
        "          tepoch.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "          for inputs, labels in tepoch:\n",
        "              inputs = inputs.to(device)\n",
        "              labels = labels.to(device)\n",
        "              # forward inputs and get output\n",
        "              optimizer.zero_grad()\n",
        "              outputs = model(inputs)\n",
        "              _, preds = torch.max(outputs, 1)\n",
        "              loss = criterion(outputs, labels)\n",
        "              # get loss value and update the network weights\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              running_loss += loss.item() * inputs.size(0)\n",
        "              running_corrects += torch.sum(preds == labels.data)\n",
        "              tepoch.set_postfix(loss=loss.item())\n",
        "      # epoch_loss = running_loss / len(train_data)\n",
        "      # epoch_acc = running_corrects / len(train_data) * 100.\n",
        "      # acc_train.append(epoch_acc)\n",
        "      #print('[Train #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() -start_time))\n",
        "\n",
        "      with torch.no_grad():\n",
        "          running_loss = 0.\n",
        "          running_corrects = 0\n",
        "          for inputs, labels in test_loader:\n",
        "              inputs = inputs.to(device)\n",
        "              labels = labels.to(device)\n",
        "              outputs = model(inputs)\n",
        "              _, preds = torch.max(outputs, 1)\n",
        "              loss = criterion(outputs, labels)\n",
        "              running_loss += loss.item() * inputs.size(0)\n",
        "              running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "          epoch_loss = running_loss / len(test_set)\n",
        "          epoch_acc = running_corrects / len(test_set) * 100.\n",
        "          acc_valid.append(epoch_acc)\n",
        "          print('[Valid #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time()- start_time))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbDabna5Epq0",
        "outputId": "695fe072-b8f1-421d-8374-28c9e552cbf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 running\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|██████████| 123/123 [00:38<00:00,  3.21batch/s, loss=1.38]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Valid #0] Loss: 0.9983 Acc: 66.6327% Time: 48.3460s\n",
            "Epoch 1 running\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|██████████| 123/123 [00:39<00:00,  3.09batch/s, loss=0.932]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Valid #1] Loss: 0.9169 Acc: 67.1429% Time: 96.9011s\n",
            "Epoch 2 running\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|██████████| 123/123 [00:40<00:00,  3.00batch/s, loss=1.35]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Valid #2] Loss: 0.8361 Acc: 69.7959% Time: 147.7072s\n",
            "Epoch 3 running\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20: 100%|██████████| 123/123 [00:38<00:00,  3.21batch/s, loss=0.626]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Valid #3] Loss: 0.7224 Acc: 76.9388% Time: 195.9700s\n",
            "Epoch 4 running\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/20: 100%|██████████| 123/123 [00:39<00:00,  3.11batch/s, loss=0.552]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Valid #4] Loss: 0.6797 Acc: 78.1633% Time: 244.5653s\n",
            "Epoch 5 running\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/20: 100%|██████████| 123/123 [00:39<00:00,  3.11batch/s, loss=0.607]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Valid #5] Loss: 0.6607 Acc: 79.1837% Time: 293.8449s\n",
            "Epoch 6 running\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/20: 100%|██████████| 123/123 [00:38<00:00,  3.18batch/s, loss=0.912]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Valid #6] Loss: 0.6209 Acc: 79.0816% Time: 342.4615s\n",
            "Epoch 7 running\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/20: 100%|██████████| 123/123 [00:39<00:00,  3.12batch/s, loss=0.625]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Valid #7] Loss: 0.5873 Acc: 80.4082% Time: 391.0803s\n",
            "Epoch 8 running\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/20: 100%|██████████| 123/123 [00:39<00:00,  3.11batch/s, loss=0.547]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Valid #8] Loss: 0.7201 Acc: 75.5102% Time: 440.2535s\n",
            "Epoch 9 running\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/20: 100%|██████████| 123/123 [00:38<00:00,  3.16batch/s, loss=0.509]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Valid #9] Loss: 0.5738 Acc: 81.3265% Time: 489.0254s\n",
            "Epoch 10 running\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/20: 100%|██████████| 123/123 [00:39<00:00,  3.11batch/s, loss=0.533]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Valid #10] Loss: 0.5808 Acc: 80.9184% Time: 537.7052s\n",
            "Epoch 11 running\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/20: 100%|██████████| 123/123 [00:39<00:00,  3.12batch/s, loss=0.557]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Valid #11] Loss: 0.5575 Acc: 82.4490% Time: 586.8092s\n",
            "Epoch 12 running\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/20: 100%|██████████| 123/123 [00:39<00:00,  3.13batch/s, loss=0.327]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Valid #12] Loss: 0.5950 Acc: 79.0816% Time: 635.9803s\n",
            "Epoch 13 running\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/20: 100%|██████████| 123/123 [00:39<00:00,  3.12batch/s, loss=0.343]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Valid #13] Loss: 0.5735 Acc: 79.5918% Time: 684.5538s\n",
            "Epoch 14 running\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/20: 100%|██████████| 123/123 [00:39<00:00,  3.10batch/s, loss=0.489]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Valid #14] Loss: 0.5554 Acc: 81.5306% Time: 733.7808s\n",
            "Epoch 15 running\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/20: 100%|██████████| 123/123 [00:39<00:00,  3.14batch/s, loss=0.38]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Valid #15] Loss: 0.5415 Acc: 81.6327% Time: 782.8755s\n",
            "Epoch 16 running\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/20: 100%|██████████| 123/123 [00:39<00:00,  3.13batch/s, loss=0.387]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Valid #16] Loss: 0.5098 Acc: 83.2653% Time: 831.5857s\n",
            "Epoch 17 running\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/20: 100%|██████████| 123/123 [00:39<00:00,  3.09batch/s, loss=0.441]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Valid #17] Loss: 0.5222 Acc: 82.1429% Time: 880.9158s\n",
            "Epoch 18 running\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/20: 100%|██████████| 123/123 [00:39<00:00,  3.13batch/s, loss=0.899]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Valid #18] Loss: 0.5155 Acc: 83.7755% Time: 930.1695s\n",
            "Epoch 19 running\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/20: 100%|██████████| 123/123 [00:42<00:00,  2.90batch/s, loss=0.621]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Valid #19] Loss: 0.5036 Acc: 83.5714% Time: 982.0368s\n"
          ]
        }
      ],
      "source": [
        "train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXa0FU6IE1so"
      },
      "outputs": [],
      "source": [
        "model_save_name = 'SER_Teess_Savee_Ravdess_classifier.pth'\n",
        "path = f\"/content/drive/MyDrive/{model_save_name}\"\n",
        "torch.save(model.state_dict(), path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjvjDepnKV8Z"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
